######################################################################################################
# --                                Pre/post-processing Settings                                  -- #
######################################################################################################

# Enables/disables TMX documents' validation.
# ---------------------------
TMX_VALIDATION = False

# Preprocessing mode:
#  - NONE: disables preprocessing
#  - SENT: sentence-splitting
#  - TOK: tokenizing
#  - TC: truecasing
#  - TOK_TC: tokenizing and truecasing
#  - SENT_TOK: sentence-splitting and tokenizing
#  - SENT_TOK_TC: sentence-splitting, tokenizing and truecasing
# ---------------------------
PREPROCESSING_MODE = SENT_TOK_TC

# Postprocessing mode:
#  - NONE: disables postprocessing
#  - DETOK: detokenizing
#  - DTC: detruecasing
#  - DTC_DETOK: detruecasing and detokenizing
# ---------------------------
POSTPROCESSING_MODE = DTC_DETOK

# List of casing models:
#  CASING_MODELS = <lang1>:<path/to/model1> \
#                  <langN>:<path/to/modelN>
# ---------------------------
CASING_MODELS = en:toolchain_resources/casing_models/jrc.mono.tok.tcm.en \
                es:toolchain_resources/casing_models/jrc.mono.tok.tcm.es \
                fr:toolchain_resources/casing_models/jrc.mono.tok.tcm.fr \
                de:toolchain_resources/casing_models/jrc.mono.tok.tcm.de \
				pt:toolchain_resources/casing_models/jrc.mono.tok.tcm.pt
				
# If UNIQ option is set to True, duplicated sentences will be removed from the corpus after
# tokenization, lowercasing or truecasing (depending on the selected preprocessing mode).
# ---------------------------
UNIQ = True

#####################################################################################################
# --                                     Alignment Settings                                      -- #
#####################################################################################################

# List of GIZA lexical translation tables:
#   DICTIONARIES = <source_lang1>2<target_lang1>:<path/to/lex1> \
#                  <source_langN>2<target_langN>:<path/to/lexN>
# ---------------------------
DICTIONARIES = en2es:toolchain_resources/translation_tables/en_es \
               es2en:toolchain_resources/translation_tables/es_en \
               en2fr:toolchain_resources/translation_tables/en_fr \
               fr2en:toolchain_resources/translation_tables/fr_en \
               es2fr:toolchain_resources/translation_tables/es_fr \
               fr2es:toolchain_resources/translation_tables/fr_es

# Document aligner used, 'cartesian' or 'lucene':
#  cartesian: all the posible alignment posibilities are computed. Fast but huge memory consuming.
#  lucene:    a lucene index is used to store documents, and for each source document, LUCENE_MAX_RESULTS alignments are computed.
#             Slower than 'cartesian', but needs less memory.
# ---------------------------
ALIGNER = cartesian

# Optimization options. After alignment process ends, it is possible to improve alignment
# using the alignment optimization algorithms below:
#  OPTIMIZER, 'none', 'bao' or 'iao':
#    none: disables the optimization process.
#    iao (Iterative Alignment Optimizer): processes several runs (see RUNS_ARG), and in each run removes duplicated targets.
#                                         Improves precission and recall.
#    bao (Best Alignment Optimizer): in case of duplicated target alignments, chooses only the best target document.
#                                    Improves precission more than 'iao', but decreases recall. This is in most cases
#                                    the best optimizer.
#  RUNS: number of runs for Iterative Alignment Optimizer.
# ---------------------------
OPTIMIZER = bao
RUNS      = 10

# Lexical translation dictionary options:
#  MAX_TRANSLATIONS: maximum number of translations used from dictionaries.
#  LEX_FILTER:       to filter translations by probability from dictionaries.
# ---------------------------
MAX_TRANSLATIONS = 5
LEX_FILTER       = False

# Longest common prefix stemming options:
#  LONGEST_COMMON_PREFIX_STEMMING: use longest common prefix stemming or not. This feature is very time consuming.
#  MIN_WORD_LENGTH:                minimum word length for longest common prefix stemming.
# ---------------------------
LONGEST_COMMON_PREFIX_STEMMING = False
MIN_WORD_LENGTH                = 3

# Weights between 0-1 can be used to increase the importance of rare words over stop-words.
# These weights are trained using the input documents. Please note that input corpus has to
# be big to train representative weights.
# ---------------------------
WEIGHTS = False

# Lucene aligner options:
#  LUCENE_MAX_RESULTS:     how many alignment candidates are provided for each source document.
#  LUCENE_MIN_WORD_LENGTH: minimum character-length for a word to be indexed and included in queries.
# ---------------------------
LUCENE_MAX_RESULTS     = 100
LUCENE_MIN_WORD_LENGTH = 4

# Threads used to compute alignment
# ---------------------------
THREADS = 4

# Path to HunAlign (http://mokk.bme.hu/resources/hunalign/)
# ---------------------------
HUNALIGN_ROOT = /elri_docker/dependencies/hunalign

# Dictionary to compute alignments. In the absence of a dictionary, it first falls back to
# sentence-length information, and then builds an automatic dictionary based on this alignment.
# Then it realigns the text in a second pass, using the automatic dictionary.
# ---------------------------
DICTIONARY = none

# If this option is set, the alignment is built in three phases.
# After an initial alignment, the algorithm heuristically adds items to the dictionary based on
# cooccurrences in the identified bisentences. Then it re-runs the alignment process based on
# this larger dictionary. This option is recommended to achieve the highest possible alignment quality.
# Use this option with caution as it approximately triples the running time while the quality
# improvement it yields are typically small.
# ---------------------------
REALIGN = True

# Sentence alignment threshold.
# ---------------------------
SENTENCE_ALIGNMENT_THRESHOLD = 0.4
